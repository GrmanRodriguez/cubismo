{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from flow_matching.path import CondOTProbPath\n",
    "from flow_matching.solver import ODESolver\n",
    "from flow_matching.utils import ModelWrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sinusoidal_positional_embedding(t, embedding_dim):\n",
    "    \"\"\"\n",
    "    Create sinusoidal embedding of scalar t.\n",
    " \n",
    "    Args:\n",
    "        t: tensor of shape [B] or [B,1], values in [0,1]\n",
    "        embedding_dim: size of the output embedding\n",
    " \n",
    "    Returns:\n",
    "        A tensor of shape [B, embedding_dim].\n",
    "    \"\"\"\n",
    "    if t.dim() == 1:\n",
    "        t = t.unsqueeze(1)  # [B, 1]\n",
    "    half_dim = embedding_dim // 2\n",
    "    # Exponential decay frequencies\n",
    "    freq = torch.exp(\n",
    "        torch.linspace(\n",
    "            0, math.log(10000), half_dim, device=t.device\n",
    "        )\n",
    "    )\n",
    "    # shape: [half_dim]\n",
    "    # Outer product -> (B, half_dim)\n",
    "    args = t * freq.unsqueeze(0)\n",
    " \n",
    "    sin = torch.sin(args)\n",
    "    cos = torch.cos(args)\n",
    "    emb = torch.cat([sin, cos], dim=1)  # (B, embedding_dim)\n",
    "    return emb\n",
    " \n",
    "class TimeLabelEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Combines a sinusoidal time embedding and a label embedding, \n",
    "    then processes them with an MLP.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, embedding_dim=128):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        # Label embedding\n",
    "        self.label_emb = nn.Embedding(num_classes, embedding_dim)\n",
    "        # A small MLP to merge time emb + label emb\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(2 * embedding_dim, embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_dim, embedding_dim),\n",
    "        )\n",
    " \n",
    "    def forward(self, t, y):\n",
    "        \"\"\"\n",
    "        t: (B,) or (B,1) time steps in [0,1]\n",
    "        y: (B,) class labels\n",
    "        \"\"\"\n",
    "        # Sinusoidal time emb\n",
    "        time_emb = get_sinusoidal_positional_embedding(t, self.embedding_dim)\n",
    "        # Label emb\n",
    "        label_emb = self.label_emb(y)  # [B, embedding_dim]\n",
    "        # Concatenate\n",
    "        combined = torch.cat([time_emb, label_emb], dim=1)  # (B, 2*embedding_dim)\n",
    "        # MLP\n",
    "        cond = self.mlp(combined)  # (B, embedding_dim)\n",
    "        return cond\n",
    " \n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Double 3x3 conv layers with conditioning-based FiLM.\n",
    " \n",
    "    in_channels -> out_channels,\n",
    "    using two (Conv -> BN -> ReLU) in sequence.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, emb_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    " \n",
    "        self.act = nn.ReLU(inplace=True)\n",
    " \n",
    "        # For FiLM parameters (scale + shift) from the embedding\n",
    "        self.scale_shift1 = nn.Linear(emb_dim, 2 * out_channels)  # gamma1, beta1\n",
    "        self.scale_shift2 = nn.Linear(emb_dim, 2 * out_channels)  # gamma2, beta2\n",
    " \n",
    "    def forward(self, x, emb):\n",
    "        \"\"\"\n",
    "        x: (B, in_channels, H, W)\n",
    "        emb: (B, emb_dim)\n",
    "        \"\"\"\n",
    "        # ----- First Conv -----\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    " \n",
    "        # Get gamma,beta from embedding for the first conv\n",
    "        gamma1, beta1 = self.scale_shift1(emb).chunk(2, dim=1)  # each (B, out_channels)\n",
    "        # Reshape/broadcast to (B, out_channels, 1, 1)\n",
    "        gamma1 = gamma1.unsqueeze(-1).unsqueeze(-1)\n",
    "        beta1 = beta1.unsqueeze(-1).unsqueeze(-1)\n",
    " \n",
    "        # FiLM: x = x * (1 + gamma) + beta\n",
    "        x = x * (1 + gamma1) + beta1\n",
    "        x = self.act(x)\n",
    " \n",
    "        # ----- Second Conv -----\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    " \n",
    "        # Get gamma,beta from embedding for the second conv\n",
    "        gamma2, beta2 = self.scale_shift2(emb).chunk(2, dim=1)\n",
    "        gamma2 = gamma2.unsqueeze(-1).unsqueeze(-1)\n",
    "        beta2 = beta2.unsqueeze(-1).unsqueeze(-1)\n",
    " \n",
    "        x = x * (1 + gamma2) + beta2\n",
    "        x = self.act(x)\n",
    "        return x\n",
    " \n",
    "class Down(nn.Module):\n",
    "    \"\"\"\n",
    "    Downscale by factor of 2 (MaxPool) then DoubleConv with conditioning.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, emb_dim):\n",
    "        super().__init__()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv = DoubleConv(in_channels, out_channels, emb_dim)\n",
    " \n",
    "    def forward(self, x, emb):\n",
    "        x = self.pool(x)\n",
    "        x = self.conv(x, emb)\n",
    "        return x\n",
    " \n",
    " \n",
    "class Up(nn.Module):\n",
    "    \"\"\"\n",
    "    Upscale by factor of 2 (ConvTranspose2d), then concat the skip connection,\n",
    "    then DoubleConv with conditioning.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, emb_dim, bilinear=False):\n",
    "        super().__init__()\n",
    "        self.bilinear = bilinear\n",
    "        if bilinear:\n",
    "            self.up = nn.Sequential(\n",
    "                nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1),\n",
    "            )\n",
    "        else:\n",
    "            # Classic transposed conv\n",
    "            self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    " \n",
    "        self.conv = DoubleConv(2 * out_channels, out_channels, emb_dim)\n",
    " \n",
    "    def forward(self, x1, x2, emb):\n",
    "        \"\"\"\n",
    "        x1: the 'lower' resolution feature\n",
    "        x2: the skip-connection feature from earlier in the encoder\n",
    "        emb: the conditioning embedding\n",
    "        \"\"\"\n",
    "        x1 = self.up(x1)\n",
    " \n",
    "        # Handle any dimension mismatches (odd input sizes)\n",
    "        diffY = x2.size(2) - x1.size(2)\n",
    "        diffX = x2.size(3) - x1.size(3)\n",
    "        if diffY != 0 or diffX != 0:\n",
    "            x1 = nn.functional.pad(\n",
    "                x1, \n",
    "                [diffX // 2, diffX - diffX // 2,\n",
    "                 diffY // 2, diffY - diffY // 2]\n",
    "            )\n",
    " \n",
    "        # Concatenate along the channel dimension\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        x = self.conv(x, emb)\n",
    "        return x\n",
    " \n",
    "class OutConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Final 1x1 convolution to get the desired output channels.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    " \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    " \n",
    " \n",
    "class UNetCond(nn.Module):\n",
    "    \"\"\"\n",
    "    U-Net that accepts:\n",
    "      - x: the input image/tensor\n",
    "      - t: a scalar time in [0,1] (for diffusion steps, etc.)\n",
    "      - y: a class label for conditional generation\n",
    " \n",
    "    Channels double as we go down, then halve as we go up, \n",
    "    with FiLM-like conditioning in each block based on (t,y).\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes=10,\n",
    "        in_channels=3,\n",
    "        out_channels=3,\n",
    "        base_channels=64,\n",
    "        emb_dim=128,\n",
    "        bilinear=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # For time + label conditioning\n",
    "        self.time_label_emb = TimeLabelEmbedding(num_classes, embedding_dim=emb_dim)\n",
    " \n",
    "        # Encoder/down\n",
    "        self.inc = DoubleConv(in_channels, base_channels, emb_dim)\n",
    "        self.down1 = Down(base_channels, base_channels * 2, emb_dim)\n",
    "        self.down2 = Down(base_channels * 2, base_channels * 4, emb_dim)\n",
    "        self.down3 = Down(base_channels * 4, base_channels * 8, emb_dim)\n",
    " \n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(base_channels * 8, base_channels * 16 // factor, emb_dim)\n",
    " \n",
    "        # Decoder/up\n",
    "        self.up1 = Up(base_channels * 16, base_channels * 8 // factor, emb_dim, bilinear)\n",
    "        self.up2 = Up(base_channels * 8, base_channels * 4 // factor, emb_dim, bilinear)\n",
    "        self.up3 = Up(base_channels * 4, base_channels * 2 // factor, emb_dim, bilinear)\n",
    "        self.up4 = Up(base_channels * 2, base_channels, emb_dim, bilinear)\n",
    "        self.outc = OutConv(base_channels, out_channels)\n",
    " \n",
    "    def forward(self, x, t, y):\n",
    "        \"\"\"\n",
    "        x: (B, in_channels, H, W)\n",
    "        t: (B,) or (B,1) time in [0,1]\n",
    "        y: (B,) class labels\n",
    "        \"\"\"\n",
    "        # 1) Get combined time+label embedding\n",
    "        emb = self.time_label_emb(t, y)  # (B, emb_dim)\n",
    " \n",
    "        # 2) Encoder\n",
    "        x1 = self.inc(x, emb)\n",
    "        x2 = self.down1(x1, emb)\n",
    "        x3 = self.down2(x2, emb)\n",
    "        x4 = self.down3(x3, emb)\n",
    "        x5 = self.down4(x4, emb)\n",
    " \n",
    "        # 3) Decoder\n",
    "        x = self.up1(x5, x4, emb)\n",
    "        x = self.up2(x, x3, emb)\n",
    "        x = self.up3(x, x2, emb)\n",
    "        x = self.up4(x, x1, emb)\n",
    " \n",
    "        # 4) Final output\n",
    "        logits = self.outc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_cifar = torchvision.datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=torchvision.transforms.Compose([torchvision.transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_celeba = torchvision.datasets.CelebA(root=\"./data\", split=\"all\", download=True, transform=torchvision.transforms.Compose([torchvision.transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_flow_matching_loss(flow_model, x, y, path):\n",
    "    x_0 = torch.randn_like(x).cuda()\n",
    " \n",
    "    t = torch.rand(x.shape[0]).to(\"cuda\")\n",
    " \n",
    "    path_sample = path.sample(t = t, x_0=x_0, x_1=x)\n",
    " \n",
    "    optimal_flow = path_sample.dx_t\n",
    "    predicted_flow = flow_model(path_sample.x_t, path_sample.t, y)\n",
    " \n",
    "    return (predicted_flow - optimal_flow).square().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrappedModel(ModelWrapper):\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor, y):\n",
    "        return self.model(x, torch.full(x.shape[:1], fill_value=t, device=\"cuda\"), y.to(\"cuda\"))\n",
    " \n",
    "@torch.no_grad()\n",
    "def run_flow_discrete(flow_model, starting_point, t_initial, t_final, labels, device=\"cuda\"):\n",
    "    solver = ODESolver(flow_model)\n",
    "    times = torch.linspace(t_initial, t_final, 10)\n",
    "    return solver.sample(x_init = starting_point, method=\"euler\", time_grid=times, step_size=0.1, y=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = \"celeba\"\n",
    "checkpoint_path = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_type == \"cifar\":\n",
    "    num_epochs = 10000\n",
    "    lr = 5e-3\n",
    "    batch_size = 768\n",
    "    loader_num_workers = 2\n",
    "    eval_and_checkpoint_every=50\n",
    "else:\n",
    "    num_epochs = 16\n",
    "    lr = 5e-3\n",
    "    batch_size = 20\n",
    "    loader_num_workers = 2\n",
    "    eval_and_checkpoint_every=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNetCond(num_classes=10 if data_type == \"cifar\" else 2, in_channels=3, out_channels=3, base_channels=64, emb_dim=128, bilinear=False)\n",
    "\n",
    "if checkpoint_path:\n",
    "    model.load_state_dict(torch.load(checkpoint_path))\n",
    "\n",
    "path = CondOTProbPath()\n",
    "\n",
    "wrapped_model = WrappedModel(model).cuda()\n",
    "wrapped_model.eval()\n",
    "\n",
    "size = [1,3,32,32] if data_type == \"cifar\" else [1,3,218,177]\n",
    "random_label = random.randint(0, 9) if data_type == \"cifar\" else random.randint(0,1)\n",
    "\n",
    "output = run_flow_discrete(wrapped_model, torch.randn(size).cuda(), 0, 1, torch.tensor([random_label]).cuda())\n",
    "image_array = output.squeeze(0).permute(1,2,0).cpu().numpy()\n",
    "image = Image.fromarray((image_array * 255).astype(np.uint8))\n",
    " \n",
    "display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_label_to_gender_label(target):\n",
    "    \"\"\"Convert gender label to 1 for women and 0 for men.\"\"\"\n",
    "    return torch.tensor(1) if target[20].item() == -1 else torch.tensor(0)\n",
    "\n",
    "def collate_by_gender(batch):\n",
    "    images, targets = zip(*batch)\n",
    "    transformed_targets = [convert_label_to_gender_label(target) for target in targets]\n",
    "    return torch.stack(images), torch.stack(transformed_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_type == \"cifar\":\n",
    "    train_loader = DataLoader(train_dataset_cifar, batch_size=batch_size, shuffle=True, num_workers=loader_num_workers)\n",
    "else:\n",
    "    train_loader = DataLoader(train_dataset_celeba, batch_size=batch_size, shuffle=True, num_workers=loader_num_workers, collate_fn=collate_by_gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints_base_name = f\"{data_type}_{time.time():.0f}\"\n",
    "\n",
    "model.cuda()\n",
    "model.train()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "amount_batches = len(train_loader)\n",
    "\n",
    "\n",
    "epoch_progress_bar = tqdm(total=num_epochs, desc=\"Epoch\", leave=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    batch_progress_bar = tqdm(total=amount_batches, desc=\"Batch\", leave=True)\n",
    "    for batch_index, (images, labels) in enumerate(train_loader):\n",
    "        model.zero_grad()\n",
    "        loss = conditional_flow_matching_loss(model, images.cuda(), labels.cuda(), path)           \n",
    "        if torch.isnan(loss):\n",
    "            print(\"loss reached NaN. Early stopping.\")\n",
    "            raise ValueError(\"loss reached NaN\")\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_progress_bar.update(1)\n",
    "\n",
    "    batch_progress_bar.close()\n",
    "    epoch_progress_bar.set_description(f\"Epoch (Loss = {loss.item():.6f})\")\n",
    "    epoch_progress_bar.update(1)\n",
    "    \n",
    "    if (epoch + 1) % eval_and_checkpoint_every == 0 or (epoch + 1) == num_epochs:\n",
    "        checkpoint_directory = f\"./checkpoints/{checkpoints_base_name}\"\n",
    "        checkpoint_name = f\"{checkpoint_directory}/epoch={epoch+1}_loss={loss.item():.6f}.ckpt\"\n",
    "        print(f\"Saving {checkpoint_name}\")\n",
    "        os.makedirs(checkpoint_directory, exist_ok=True)\n",
    "        torch.save(model.state_dict(), checkpoint_name)\n",
    "\n",
    "        print(f\"Evaluating model at checkpoint {checkpoint_name}\")\n",
    "        model.eval()\n",
    "        wrapped_model = WrappedModel(model)\n",
    "        wrapped_model.eval()\n",
    "\n",
    "        size = [1,3,32,32] if data_type == \"cifar\" else [1,3,218,177]\n",
    "        random_label = random.randint(0, 9) if data_type == \"cifar\" else random.randint(0,1)\n",
    "        output = run_flow_discrete(wrapped_model, torch.randn(size).cuda(), 0, 1, torch.tensor([random_label]).cuda()) \n",
    "        image_array = output.squeeze(0).permute(1,2,0).cpu().numpy()\n",
    "        image = Image.fromarray((image_array * 255).astype(np.uint8))\n",
    "        display(image)\n",
    "\n",
    "        model.train()\n",
    "\n",
    "batch_progress_bar.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapped_model = WrappedModel(model).cuda()\n",
    "# wrapped_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size = [1,3,32,32] if data_type == \"cifar\" else [1,3,218,177]\n",
    "# random_label = random.randint(0, 9) if data_type == \"cifar\" else random.randint(0,1)\n",
    "\n",
    "# output = run_flow_discrete(wrapped_model, torch.randn(size).cuda(), 0, 1, torch.tensor([random_label]).cuda())\n",
    "# image_array = output.squeeze(0).permute(1,2,0).cpu().numpy()\n",
    "# image = Image.fromarray((image_array * 255).astype(np.uint8))\n",
    "# print(random_label)\n",
    "# display(image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image_gen_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
